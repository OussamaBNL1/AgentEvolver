import torch
import verl.utils.torch_functional as verl_F
from openai import AsyncOpenAI
import os
import json
from pathlib import Path
from loguru import logger
import time
import traceback
from tqdm import tqdm
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Optional, Literal
import threading
from dataclasses import dataclass, asdict

__all__ = [
    "evaluate_step_flags_parallel",    # å¹¶è¡Œç‰ˆæœ¬çš„stepè¯„ä¼°
    "apply_step_mask_vectorized",      # å‘é‡åŒ–çš„maskåº”ç”¨
    "ParallelSemanticProcessor",       # ç»Ÿä¸€çš„å¤„ç†å™¨ç±»
]

@dataclass
class EvaluationTask:
    """è¯„ä¼°ä»»åŠ¡çš„æ•°æ®ç»“æ„"""
    sample_idx: int
    step_idx: int
    query: str
    rollout: str
    step_text: str
    overall_adv: float

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœçš„æ•°æ®ç»“æ„"""
    sample_idx: int
    step_idx: int
    is_good: bool
    response_time: float

@dataclass
class EvaluationRecord:
    """è¯„ä¼°è®°å½•çš„æ•°æ®ç»“æ„ï¼Œç”¨äºä¿å­˜åˆ°æ–‡ä»¶"""
    sample_idx: int
    step_idx: int
    query: str
    rollout: str
    step_text: str
    overall_adv: float
    llm_input_messages: List[Dict]
    llm_raw_output: str
    llm_parsed_result: bool  # True for GOOD, False for BAD
    response_time: float
    timestamp: float
    model_name: str
    evaluation_type: str
    global_step: Optional[int] = None
    epoch: Optional[str] = None
# =========================================================
# Added: rollout parsing & batch-eval prompt utilities
# =========================================================
import re 

def parse_rollout_to_steps(rollout: str) -> List[Dict[str, str]]:
    """
    å°†åŒ…å«
        ... assistant\n<action text>\nuser\n<observation text> ...
    çš„é•¿ä¸² rollout æ‹†æˆæ­¥éª¤åˆ—è¡¨ã€‚
    è¿”å›:
        [
            {"action": ..., "observation": ...},      # step 0
            {"action": ..., "observation": ...},      # step 1
            ...
        ]
    è§£æé€»è¾‘ï¼š
        - æŒ‰æ ‡ç­¾ ('assistant' / 'user') åˆ†å‰²
        - æ¯é‡åˆ° assistant è§†ä¸ºæ–° step.action
        - è‹¥å…¶åç´§è·Ÿ userï¼Œåˆ™è®°ä¸º step.observation
    """
    # æŠŠæ ‡ç­¾å’Œæ­£æ–‡ä¸€æ¬¡æ€§æ‹†å¼€ï¼šparts = [pre, tag1, txt1, tag2, txt2, ...]
    parts = re.split(r'\n(assistant|user)\n', rollout, flags=re.I)

    # è‹¥ rollout ä¸æ˜¯ä»¥æ ‡ç­¾å¼€å¤´ï¼ŒæŠŠå‰ç½®æ–‡æœ¬è§†ä¸º assistant è¡Œä¸º
    if parts and parts[0].strip():
        parts = ['assistant', parts[0]] + parts[1:]

    steps: List[Dict[str, str]] = []
    i = 0
    while i < len(parts) - 1:
        role, text = parts[i].lower(), parts[i + 1]
        if role == 'assistant':
            action = text.strip()
            observation = ''
            # å¦‚æœä¸‹ä¸€å¯¹æ˜¯ userï¼Œåˆ™åŠ å…¥ observation
            if i + 2 < len(parts) - 1 and parts[i + 2].lower() == 'user':
                observation = parts[i + 3].strip()
                i += 2  # è·³è¿‡ user æ®µ
            steps.append({"action": action, "observation": observation})
        # è·³åˆ°ä¸‹ä¸€ tag
        i += 2
    return steps


# =========================================================
# 2) build_prompt â€” æŒ‰æ­¥éª¤åˆ—è¡¨ç”Ÿæˆæ‰¹é‡è¯„ä¼° prompt
# =========================================================
def build_batch_evaluation_prompt(
    query: str,
    steps: List[Dict[str, str]],
    overall_adv: float,
    max_step_chars: int = 2000,
) -> List[dict]:
    """
    è¾“å…¥ç»“æ„åŒ– stepsï¼Œè¾“å‡º ChatCompletion æ‰€éœ€ messagesï¼ˆsystem+userï¼‰
    """
    polarity = "positive" if overall_adv > 0 else "negative"
    sys_msg = (
        "You are an expert reward-model evaluator.\n"
        "For **each step** decide whether it helps (GOOD) or hurts (BAD) solving the userâ€™s task.\n"
        "Reply exactly in the REQUIRED OUTPUT FORMAT and nothing else."
    )

    def _trim(s: str) -> str:
        return s if len(s) <= max_step_chars else s[:max_step_chars] + "\nâ€¦"

    # --- å¤´éƒ¨ ---
    user_parts = [
        f"**OVERALL ADVANTAGE {overall_adv:+.4f} ({polarity})**",
        "",
        "### USER QUERY",
        query,
        "",
        f"### TRAJECTORY  (total {len(steps)} steps)",
    ]

    # --- é€æ­¥æ‹¼è£… ---
    for idx, st in enumerate(steps):
        block = [
            f"=== STEP {idx} ===",
            "<|ACTION|>",
            _trim(st['action']),
            "<|END|>",
        ]
        if st['observation']:
            block += [
                "<|OBSERVATION|>",
                _trim(st['observation']),
                "<|END|>",
            ]
        user_parts.append("\n".join(block))

    # --- æ”¶å°¾ + è¾“å‡ºæ ¼å¼è¯´æ˜ ---
    user_parts += [
        "",
        "---",
        "For each step, think: *Does this help solve the task?*",
        "",
        "REQUIRED OUTPUT FORMAT:",
        "Step 0 Analysis: <your reasoning>",
        "Step 0 Judgment: GOOD/BAD",
        "",
        "Step 1 Analysis: <your reasoning>",
        "Step 1 Judgment: GOOD/BAD",
        "",
        "[â€¦continue for all stepsâ€¦]",
    ]

    return [
        {"role": "system", "content": sys_msg},
        {"role": "user", "content": "\n".join(user_parts)},
    ]
def build_batch_evaluation_prompt_from_rollout(query: str,
                                               rollout: str,
                                               overall_adv: float,
                                               max_step_chars: int = 2000):
    steps = parse_rollout_to_steps(rollout)
    return build_batch_evaluation_prompt(query, steps, overall_adv, max_step_chars)

def parse_batch_evaluation_result(response: str, num_steps: int):
    numbered = {}
    for m in re.finditer(r"Step\s+(\d+)\s+Judgment:\s*(GOOD|BAD)", response, flags=re.I):
        numbered[int(m.group(1))] = m.group(2).upper() == "GOOD"
    if len(numbered) == num_steps:
        return [numbered[i] for i in range(num_steps)]
    flags = re.findall(r"\b(GOOD|BAD)\b", response.upper())
    if len(flags) >= num_steps:
        return [flag == "GOOD" for flag in flags[:num_steps]]
    raise ValueError("Could not parse evaluation result")


# å…¨å±€å˜é‡å­˜å‚¨vLLMæ¨¡å‹å’Œtokenizerï¼ˆç”¨äºæœ¬åœ°è¯„ä¼°ï¼‰
_vllm_model = None
_vllm_tokenizer = None
_model_lock = threading.Lock()

def _get_overall_advantage(advantages_tensor, mask=None):
    """
    ä»advantages tensorä¸­è·å–overall advantageå€¼
    åœ¨GRPOä¸­ï¼Œæ‰€æœ‰æœ‰æ•ˆtokenå…±äº«ä¸€ä¸ªadvantageï¼Œæˆ‘ä»¬éœ€è¦æ­£ç¡®æå–è¿™ä¸ªå€¼
    
    Args:
        advantages_tensor: advantage tensor, shape (resp_len,) 
        mask: æ ‡è¯†éœ€è¦è®­ç»ƒçš„tokenä½ç½®çš„maskï¼Œshape (resp_len,)
              å¯ä»¥æ˜¯loss_maskæˆ–response_maskï¼Œå–å†³äºå¤–éƒ¨ä¼ å…¥
    
    Returns:
        float: æå–åˆ°çš„overall advantageå€¼
    """
    if advantages_tensor.dim() == 0:  # scalar
        return advantages_tensor.item()
    
    if advantages_tensor.dim() == 1:  # shape: (resp_len,)
        # ä¼˜å…ˆä½¿ç”¨maskæ¥æå–æœ‰æ•ˆadvantage
        if mask is not None:
            valid_advantages = advantages_tensor[mask.bool()]
            if len(valid_advantages) > 0:
                # åœ¨GRPOä¸­ï¼Œæ‰€æœ‰æœ‰æ•ˆtokençš„advantageåº”è¯¥ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªå³å¯
                return valid_advantages[0].item()
            else:
                # maskä¸­æ²¡æœ‰æœ‰æ•ˆtokenï¼Œè¿”å›0
                return 0.0
        else:
            # fallback: æ²¡æœ‰maskæ—¶ï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªéé›¶å€¼
            non_zero_mask = torch.abs(advantages_tensor) > 1e-8
            if non_zero_mask.any():
                return advantages_tensor[non_zero_mask][0].item()
            else:
                return 0.0
    
    # å…¶ä»–ç»´åº¦ä¸æ”¯æŒ
    raise ValueError(f"Unsupported advantages_tensor shape: {advantages_tensor.shape}")


def _save_evaluation_record(record: EvaluationRecord, save_dir: Optional[str] = None):
    """
    ä¿å­˜è¯„ä¼°è®°å½•åˆ°æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
    
    Args:
        record: è¯„ä¼°è®°å½•
        save_dir: ä¿å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜
    """
    if save_dir is None:
        return
    
    try:
        # åˆ›å»ºåŸºç¡€ä¿å­˜ç›®å½•
        base_save_path = Path(save_dir)
        base_save_path.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®global_stepåˆ›å»ºå­ç›®å½•ï¼ˆæ¯ä¸ªstepä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰
        if record.global_step is not None:
            step_subdir = f"step_{record.global_step:06d}"
        else:
            step_subdir = "step_unknown"
        
        step_save_path = base_save_path / step_subdir
        step_save_path.mkdir(parents=True, exist_ok=True)
        
        # æ„é€ æ–‡ä»¶åï¼šåŒ…å«global_stepã€sample_idxã€step_idx
        timestamp_str = f"{record.timestamp:.3f}".replace('.', '_')
        global_step_str = f"step{record.global_step:06d}" if record.global_step is not None else "nostep"
        filename = f"{global_step_str}_sample{record.sample_idx:03d}_step{record.step_idx:02d}_{timestamp_str}.json"
        
        file_path = step_save_path / filename
        
        # å°†è®°å½•è½¬æ¢ä¸ºå­—å…¸å¹¶ä¿å­˜
        record_dict = asdict(record)
        
        # æ·»åŠ ä¸€äº›é¢å¤–çš„å…ƒæ•°æ®
        record_dict["_metadata"] = {
            "save_time": time.time(),
            "step_directory": step_subdir,
            "file_name": filename,
            "full_path": str(file_path)
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(record_dict, f, ensure_ascii=False, indent=2)
        
        # è®°å½•ä¿å­˜æˆåŠŸçš„æ—¥å¿—
        print(f"[record_save] âœ… Saved evaluation record: {step_subdir}/{filename}")
            
    except Exception as e:
        print(f"[record_save] âŒ Failed to save evaluation record: {e}")
        print(f"[record_save] ğŸ“ Attempted path: {save_dir}")
        print(f"[record_save] ğŸ“„ Record details: sample_{record.sample_idx}_step_{record.step_idx}")
        
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„é”™è¯¯è®°å½•
        try:
            error_save_path = Path(save_dir)
            error_save_path.mkdir(parents=True, exist_ok=True)
            
            error_filename = f"ERROR_sample{record.sample_idx:03d}_step{record.step_idx:02d}_{time.time():.0f}.json"
            error_file_path = error_save_path / error_filename
            
            error_record = {
                "error": str(e),
                "sample_idx": record.sample_idx,
                "step_idx": record.step_idx,
                "global_step": record.global_step,
                "timestamp": record.timestamp,
                "attempted_save_dir": save_dir
            }
            
            with open(error_file_path, 'w', encoding='utf-8') as f:
                json.dump(error_record, f, ensure_ascii=False, indent=2)
                
            print(f"[record_save] ğŸ†˜ Saved error record: {error_filename}")
            
        except Exception as e2:
            print(f"[record_save] ğŸ’¥ Failed to save error record: {e2}")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# APIè¯„ä¼°ï¼ˆOpenAIå…¼å®¹ï¼‰- å¢å¼ºçš„é‡è¯•æœºåˆ¶
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

async def _async_safe_query(client: AsyncOpenAI, 
                           model: str, 
                           messages: list[dict], 
                           semaphore: asyncio.Semaphore,
                           max_retries: int = 200) -> str:
    """
    å¼‚æ­¥å®‰å…¨çš„APIè°ƒç”¨ï¼Œå¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼Œä¸“é—¨å¤„ç†429é”™è¯¯
    æ”¯æŒqwq-plusç­‰æ€è€ƒæ¨¡å‹çš„æµå¼è¾“å‡ºï¼Œåªè¿”å›æœ€ç»ˆç­”æ¡ˆ
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        model: æ¨¡å‹åç§°
        messages: æ¶ˆæ¯åˆ—è¡¨
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤200æ¬¡
    
    Returns:
        APIå“åº”å†…å®¹ï¼ˆåªåŒ…å«æœ€ç»ˆç­”æ¡ˆï¼Œä¸åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
    """
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯qwq-plusæˆ–å…¶ä»–æ€è€ƒæ¨¡å‹
                is_thinking_model = model.lower() in ["qwq-plus", "qwen3-30b-a3b-thinking-2507", "qwen3-235b-a22b-thinking-2507"]
                
                if is_thinking_model:
                    # å¯¹äºæ€è€ƒæ¨¡å‹ï¼Œä½¿ç”¨æµå¼è¾“å‡º
                    print(f"[API] Using streaming mode for thinking model: {model}")
                    
                    response = await client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=0.0,
                        extra_body={"enable_thinking": True},
                        stream=True,
                        max_tokens=50,  # å¯¹äºGOOD/BADè¯„ä¼°ï¼Œ50ä¸ªtokenè¶³å¤Ÿäº†
                    )
                    
                    # æ”¶é›†æµå¼å“åº”ï¼Œåªä¿ç•™æœ€ç»ˆç­”æ¡ˆ
                    answer_content = ""
                    reasoning_content = ""  # æ€è€ƒè¿‡ç¨‹ï¼ˆç”¨äºè°ƒè¯•ï¼Œä¸è¿”å›ï¼‰
                    is_answering = False
                    
                    async for chunk in response:
                        if not chunk.choices:
                            continue
                        
                        delta = chunk.choices[0].delta
                        
                        # æ”¶é›†æ€è€ƒå†…å®¹ï¼ˆä½†ä¸ä½¿ç”¨ï¼‰
                        if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                            reasoning_content += delta.reasoning_content
                        
                        # æ”¶é›†æœ€ç»ˆç­”æ¡ˆå†…å®¹
                        if hasattr(delta, "content") and delta.content:
                            if not is_answering:
                                is_answering = True
                            answer_content += delta.content
                    
                    # è¿”å›æœ€ç»ˆç­”æ¡ˆ
                    final_answer = answer_content.strip()
                    print(f"[API] Thinking model response - Answer: '{final_answer[:50]}...' (reasoning length: {len(reasoning_content)})")
                    return final_answer
                    
                else:
                    # å¯¹äºéæ€è€ƒæ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                    response = await client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=0.0,
                        timeout=30,
                        max_tokens=10,
                    )
                    return response.choices[0].message.content.strip()
                
            except Exception as e:
                last_exception = e
                error_str = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                is_rate_limit_error = (
                    "429" in error_str or 
                    "rate limit" in error_str or
                    "limit_requests" in error_str or
                    "exceeded your current requests" in error_str
                )
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–å¯é‡è¯•çš„é”™è¯¯
                is_retryable_error = (
                    "timeout" in error_str or
                    "connection" in error_str or
                    "500" in error_str or
                    "502" in error_str or
                    "503" in error_str or
                    "504" in error_str
                )
                
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    if is_rate_limit_error:
                        # 429é”™è¯¯ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œä½†æœ‰ä¸Šé™
                        # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼š1ç§’ï¼Œæ¯æ¬¡ç¿»å€ï¼Œæœ€å¤§60ç§’
                        base_wait = min(1.0 * (2 ** min(attempt, 6)), 60.0)
                        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼Œé¿å…æ‰€æœ‰è¯·æ±‚åŒæ—¶é‡è¯•
                        import random
                        jitter = random.uniform(0.1, 0.3) * base_wait
                        wait_time = base_wait + jitter
                        
                        print(f"[API Retry] 429 Rate limit hit, attempt {attempt + 1}/{max_retries}, waiting {wait_time:.2f}s")
                        await asyncio.sleep(wait_time)
                        
                    elif is_retryable_error:
                        # å…¶ä»–å¯é‡è¯•é”™è¯¯ï¼šè¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                        wait_time = min(2.0 * (attempt + 1), 10.0)
                        print(f"[API Retry] Retryable error, attempt {attempt + 1}/{max_retries}, waiting {wait_time:.2f}s: {e}")
                        await asyncio.sleep(wait_time)
                        
                    else:
                        # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç«‹å³å¤±è´¥
                        print(f"[API Error] Non-retryable error, failing immediately: {e}")
                        break
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    if is_rate_limit_error:
                        print(f"[API Error] Rate limit exceeded after {max_retries} attempts")
                    else:
                        print(f"[API Error] Max retries ({max_retries}) exceeded: {e}")
        
        raise last_exception

async def _evaluate_single_task_api(client: AsyncOpenAI,
                                  model_name: str,
                                  task: EvaluationTask,
                                  semaphore: asyncio.Semaphore,
                                  max_retries: int = 200,
                                  save_dir: Optional[str] = None,
                                  global_step: Optional[int] = None,
                                  epoch: Optional[str] = None) -> EvaluationResult:
    """
    ä½¿ç”¨APIè¯„ä¼°å•ä¸ªä»»åŠ¡ï¼Œå¢å¼ºé‡è¯•æœºåˆ¶ï¼Œå¹¶ä¿å­˜è¯„ä¼°è®°å½•
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        model_name: æ¨¡å‹åç§°
        task: è¯„ä¼°ä»»åŠ¡
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        save_dir: ä¿å­˜ç›®å½•
        global_step: å…¨å±€æ­¥æ•°
        epoch: è®­ç»ƒè½®æ¬¡
    """
    start_time = time.time()
    
    try:
        messages = _build_prompt(task.query, task.rollout, task.step_text, task.overall_adv)
        llm_raw_output = await _async_safe_query(client, model_name, messages, semaphore, max_retries)
        
        answer_upper = llm_raw_output.upper()
        is_good = answer_upper.startswith("G") or "GOOD" in answer_upper
        
        response_time = time.time() - start_time
        
        # ä¿å­˜è¯„ä¼°è®°å½•
        if save_dir:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒæ¨¡å‹
            is_thinking_model = model_name.lower() in ["qwq-plus", "qwen3-30b-a3b-thinking-2507", "qwen3-235b-a22b-thinking-2507"]
            
            record = EvaluationRecord(
                sample_idx=task.sample_idx,
                step_idx=task.step_idx,
                query=task.query,
                rollout=task.rollout,
                step_text=task.step_text,
                overall_adv=task.overall_adv,
                llm_input_messages=messages,
                llm_raw_output=llm_raw_output,
                llm_parsed_result=is_good,
                response_time=response_time,
                timestamp=time.time(),
                model_name=f"{model_name}{'_thinking' if is_thinking_model else ''}",
                evaluation_type="api",
                global_step=global_step,
                epoch=epoch
            )
            _save_evaluation_record(record, save_dir)
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )
        
    except Exception as e:
        response_time = time.time() - start_time
        print(f"[parallel_eval] Failed to evaluate sample {task.sample_idx}, step {task.step_idx} after all retries: {e}")
        
        # å¤±è´¥æ—¶ä½¿ç”¨éšæœºfallback
        import random
        is_good = random.choice([True, False])
        
        # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜è®°å½•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if save_dir:
            record = EvaluationRecord(
                sample_idx=task.sample_idx,
                step_idx=task.step_idx,
                query=task.query,
                rollout=task.rollout,
                step_text=task.step_text,
                overall_adv=task.overall_adv,
                llm_input_messages=_build_prompt(task.query, task.rollout, task.step_text, task.overall_adv),
                llm_raw_output=f"ERROR: {str(e)}",
                llm_parsed_result=is_good,
                response_time=response_time,
                timestamp=time.time(),
                model_name=model_name,
                evaluation_type="api",
                global_step=global_step,
                epoch=epoch
            )
            _save_evaluation_record(record, save_dir)
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç»Ÿä¸€çš„å¹¶è¡Œè¯„ä¼°æ¥å£
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

async def evaluate_step_flags_parallel(tokenizer,
                                     batch,
                                     model_name: str = "qwen-max",
                                     evaluation_type: Literal["local", "api"] = "api",
                                     max_concurrent: int = 20,
                                     batch_size_limit: int = 100,
                                     mask_tensor: torch.Tensor = None,
                                     api_max_retries: int = 200,
                                     save_dir: Optional[str] = None,
                                     global_step: Optional[int] = None,
                                     epoch: Optional[str] = None) -> Tuple[List[List[bool]], Dict]:
    """
    å¹¶è¡Œè¯„ä¼°step flagsï¼Œæ”¯æŒæœ¬åœ°æ¨¡å‹å’ŒAPIä¸¤ç§æ–¹å¼
    å¯¹äºadvantage=0çš„æ ·æœ¬è·³è¿‡è¯„ä¼°ï¼Œç›´æ¥è¿”å›GOOD
    
    Args:
        tokenizer: åˆ†è¯å™¨
        batch: æ•°æ®æ‰¹æ¬¡
        model_name: æ¨¡å‹åç§°
        evaluation_type: è¯„ä¼°ç±»å‹ï¼Œ"local"ä½¿ç”¨vLLMæœ¬åœ°æ¨¡å‹ï¼Œ"api"ä½¿ç”¨APIè°ƒç”¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        batch_size_limit: å•æ‰¹æ¬¡å¤„ç†çš„æœ€å¤§ä»»åŠ¡æ•°
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensorï¼Œshape (bs, resp_len)
                    å¯ä»¥æ˜¯loss_maskæˆ–response_maskï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
        api_max_retries: APIè°ƒç”¨çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç‰¹åˆ«ç”¨äºå¤„ç†429é”™è¯¯
        save_dir: ä¿å­˜è¯„ä¼°è®°å½•çš„ç›®å½•
        global_step: å…¨å±€æ­¥æ•°
        epoch: è®­ç»ƒè½®æ¬¡
        
    Returns:
        (flags_per_sample, stats): è¯„ä¼°ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    """
    batch_size = len(batch.batch['prompts'])
    print(f"[parallel_eval] Starting parallel evaluation for {batch_size} samples using {evaluation_type} mode")
    print(f"[parallel_eval] Model: {model_name}, API max retries: {api_max_retries}")
    if save_dir:
        print(f"[parallel_eval] Saving evaluation records to: {save_dir}")
    
    # æ£€æŸ¥å¿…è¦çš„è¾“å…¥
    if 'steps' not in batch.non_tensor_batch:
        raise ValueError("batch.non_tensor_batch['steps'] is required but not found")
    
    # æ ¹æ®è¯„ä¼°ç±»å‹åˆå§‹åŒ–
    if evaluation_type == "local":
        raise ValueError('Local evaluation via vLLM has been removed; set evaluation_type="api"')
        # åˆå§‹åŒ–vLLMæ¨¡å‹
        try:
            vllm_model, vllm_tokenizer = _initialize_vllm_model(model_name)
            api_client = None
        except Exception as e:
            print(f"[parallel_eval] Failed to initialize vLLM model, using random fallback: {e}")
            return _apply_fallback_strategy_parallel(batch), {"fallback_used": True, "error": str(e), "evaluation_type": evaluation_type}
    elif evaluation_type == "api":
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§API keyè·å–æ–¹å¼
        api_key = None
        
        # æ–¹å¼1ï¼šä»ç¯å¢ƒå˜é‡è·å–ï¼ˆæ¨èï¼‰
        api_key = os.getenv("DASHSCOPE_API_KEY")
        
        # æ–¹å¼2ï¼šå¦‚æœç¯å¢ƒå˜é‡æ²¡æœ‰ï¼Œå°è¯•ä»å…¶ä»–æ¥æºè·å–
        if not api_key:
            print("[parallel_eval] No API key found in DASHSCOPE_API_KEY environment variable")
            print("[parallel_eval] Please set: export DASHSCOPE_API_KEY='your-api-key'")
            print("[parallel_eval] Using random fallback for evaluation")
            return _apply_fallback_strategy_parallel(batch), {"fallback_used": True, "evaluation_type": evaluation_type}
        
        api_client = AsyncOpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        vllm_model = vllm_tokenizer = None
    else:
        raise ValueError(f"Unsupported evaluation_type: {evaluation_type}. Must be 'local' or 'api'")
    
    # å‡†å¤‡æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ï¼Œè·³è¿‡advantage=0çš„æ ·æœ¬
    all_tasks = []
    flags_per_sample = [[] for _ in range(batch_size)]
    skipped_samples = 0
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„mask_tensorï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥åˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
    if mask_tensor is not None:
        response_mask = mask_tensor
        print(f"[parallel_eval] Using external mask tensor with shape {mask_tensor.shape}")
        
        # éªŒè¯mask tensorçš„å½¢çŠ¶
        response_length = batch.batch["responses"].size(1)
        if response_mask.shape != (batch_size, response_length):
            raise ValueError(f"mask_tensor shape {response_mask.shape} doesn't match expected shape ({batch_size}, {response_length})")
    else:
        # ä½¿ç”¨é»˜è®¤çš„loss_mask
        response_length = batch.batch["responses"].size(1)
        response_mask = batch.batch["loss_mask"][:, -response_length:]
        print(f"[parallel_eval] Using default loss_mask")

    for sample_idx in range(batch_size):
        query = tokenizer.decode(batch.batch["prompts"][sample_idx], skip_special_tokens=True)
        rollout = tokenizer.decode(batch.batch["responses"][sample_idx], skip_special_tokens=True)
        steps = batch.non_tensor_batch["steps"][sample_idx]
        
        # ä½¿ç”¨ä¼ å…¥çš„maskæå–æ­£ç¡®çš„overall advantage
        sample_mask = response_mask[sample_idx]
        
        overall_adv = _get_overall_advantage(
            batch.batch["advantages"][sample_idx], 
            sample_mask
        )
        
        # æ–°å¢ï¼šå¦‚æœadvantageä¸º0ï¼Œç›´æ¥è®¾ç½®æ‰€æœ‰stepä¸ºGOODï¼Œè·³è¿‡APIè°ƒç”¨
        if abs(overall_adv) < 1e-8:  # ä½¿ç”¨å°çš„é˜ˆå€¼å¤„ç†æµ®ç‚¹ç²¾åº¦é—®é¢˜
            print(f"[parallel_eval] Sample {sample_idx}: advantageâ‰ˆ0 ({overall_adv:.6f}), skipping evaluation, returning all GOOD")
            flags_per_sample[sample_idx] = [True] * len(steps)  # æ‰€æœ‰stepéƒ½æ ‡è®°ä¸ºGOOD
            skipped_samples += 1
            
            # å³ä½¿è·³è¿‡è¯„ä¼°ï¼Œä¹Ÿä¿å­˜è®°å½•ï¼ˆç”¨äºåˆ†æï¼‰
            if save_dir:
                for step_idx, step_text in enumerate(steps):
                    record = EvaluationRecord(
                        sample_idx=sample_idx,
                        step_idx=step_idx,
                        query=query,
                        rollout=rollout,
                        step_text=step_text,
                        overall_adv=overall_adv,
                        llm_input_messages=[],  # ç©ºçš„ï¼Œå› ä¸ºæ²¡æœ‰è°ƒç”¨LLM
                        llm_raw_output="SKIPPED_ZERO_ADVANTAGE",
                        llm_parsed_result=True,  # é»˜è®¤ä¸ºGOOD
                        response_time=0.0,
                        timestamp=time.time(),
                        model_name=model_name,
                        evaluation_type=evaluation_type,
                        global_step=global_step,
                        epoch=epoch
                    )
                    _save_evaluation_record(record, save_dir)
            continue
        
        # ä¸ºéé›¶advantageçš„æ ·æœ¬åˆ›å»ºè¯„ä¼°ä»»åŠ¡
        for step_idx, step_text in enumerate(steps):
            task = EvaluationTask(
                sample_idx=sample_idx,
                step_idx=step_idx,
                query=query,
                rollout=rollout,
                step_text=step_text,
                overall_adv=overall_adv
            )
            all_tasks.append(task)
    
    total_tasks = len(all_tasks)
    print(f"[parallel_eval] Total tasks to process: {total_tasks}")
    print(f"[parallel_eval] Skipped {skipped_samples} samples with advantage=0")
    
    if total_tasks == 0:
        # æ‰€æœ‰æ ·æœ¬éƒ½è¢«è·³è¿‡äº†
        print("[parallel_eval] No tasks to process, all samples had advantage=0")
        if api_client:
            await api_client.close()
        return flags_per_sample, {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_api_time": 0,
            "avg_api_time": 0,
            "max_concurrent": max_concurrent,
            "fallback_used": False,
            "skipped_samples": skipped_samples,
            "evaluation_type": evaluation_type,
            "api_max_retries": api_max_retries
        }
    
    # åˆ†æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆé¿å…å†…å­˜è¿‡å¤§ï¼‰
    all_results = []
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # ä½¿ç”¨è¿›åº¦æ¡
    with tqdm(total=total_tasks, desc=f"[parallel_eval] Processing tasks ({evaluation_type})") as pbar:
        for i in range(0, total_tasks, batch_size_limit):
            batch_tasks = all_tasks[i:i + batch_size_limit]
            
            # æ ¹æ®è¯„ä¼°ç±»å‹åˆ›å»ºåç¨‹ä»»åŠ¡
            if evaluation_type == "local":
                raise ValueError('Local evaluation via vLLM has been removed; set evaluation_type="api"')
            else:  # api
                coroutines = [
                    _evaluate_single_task_api(api_client, model_name, task, semaphore, api_max_retries, save_dir, global_step, epoch)
                    for task in batch_tasks
                ]
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*coroutines, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"[parallel_eval] Task failed with exception: {result}")
                    continue
                all_results.append(result)
            
            pbar.update(len(batch_tasks))
    
    # æ•´ç†ç»“æœåˆ°å·²ç»åˆå§‹åŒ–çš„flags_per_sampleä¸­
    # æŒ‰sample_idxå’Œstep_idxæ’åº
    all_results.sort(key=lambda x: (x.sample_idx, x.step_idx))
    
    for result in all_results:
        # ä¸ºéè·³è¿‡çš„æ ·æœ¬å¡«å……ç»“æœ
        if not flags_per_sample[result.sample_idx]:  # å¦‚æœè¿˜æ˜¯ç©ºåˆ—è¡¨
            flags_per_sample[result.sample_idx] = []
        flags_per_sample[result.sample_idx].append(result.is_good)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_time = sum(r.response_time for r in all_results)
    avg_time = total_time / len(all_results) if all_results else 0
    
    stats = {
        "total_tasks": total_tasks,
        "successful_tasks": len(all_results),
        "failed_tasks": total_tasks - len(all_results),
        "total_api_time": total_time,
        "avg_api_time": avg_time,
        "max_concurrent": max_concurrent,
        "fallback_used": False,
        "skipped_samples": skipped_samples,
        "evaluation_type": evaluation_type,
        "model_name": model_name,
        "api_max_retries": api_max_retries,
        "save_dir": save_dir
    }
    
    print(f"[parallel_eval] Completed. Stats: {stats}")
    
    # æ¸…ç†èµ„æº
    if api_client:
        await api_client.close()
    
    return flags_per_sample, stats

def _apply_fallback_strategy_parallel(batch) -> List[List[bool]]:
    """å¹¶è¡Œfallbackç­–ç•¥"""
    import random
    
    flags_per_sample = []
    for steps in batch.non_tensor_batch["steps"]:
        flags = [random.choice([True, False]) for _ in steps]
        flags_per_sample.append(flags)
    
    return flags_per_sample

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# å‘é‡åŒ–çš„maskåº”ç”¨ï¼ˆä¿æŒä¸å˜ï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def apply_step_mask_vectorized(batch,
                             step_flags: List[List[bool]],
                             consistent_scale: float = 1.0,
                             pos_unconsistent_scale: float = 0.2,
                             neg_unconsistent_scale: float = -0.2,
                             mask_tensor: torch.Tensor = None) -> Dict:
    """
    å‘é‡åŒ–ç‰ˆæœ¬çš„step maskåº”ç”¨ï¼Œé¿å…åµŒå¥—å¾ªç¯
    å¯¹äºadvantage=0çš„æ ·æœ¬è·³è¿‡å¤„ç†
    """
    print(f"[vectorized_mask] Starting vectorized mask application")

    if 'step_ids' not in batch.batch:
        raise ValueError("batch.batch['step_ids'] is required but not found")

    adv = batch.batch["advantages"]              # (bs, resp_len)
    step_ids = batch.batch["step_ids"].to(adv.device)
    bs, resp_len = adv.shape

    if len(step_flags) != bs:
        raise ValueError(f"step_flags length ({len(step_flags)}) != batch size ({bs})")

    # mask é€‰æ‹©
    if mask_tensor is not None:
        response_mask = mask_tensor
        print(f"[vectorized_mask] Using external mask tensor with shape {mask_tensor.shape}")
        if response_mask.shape != (bs, resp_len):
            raise ValueError(f"mask_tensor shape {response_mask.shape} doesn't match expected shape ({bs}, {resp_len})")
    else:
        response_mask = batch.batch["loss_mask"][:, -resp_len:]
        print(f"[vectorized_mask] Using default loss_mask")

    # overall_adv per sample
    overall_advs = []
    for sample_idx in range(bs):
        sample_mask = response_mask[sample_idx]
        overall_adv = _get_overall_advantage(adv[sample_idx], sample_mask)
        overall_advs.append(overall_adv)
    overall_advs = torch.tensor(overall_advs, device=adv.device)
    overall_pos = overall_advs > 0  # (bs,)

    # init scale
    scale = torch.ones_like(adv)

    # ç»Ÿè®¡é‡
    stats = {
        "total_samples": bs,
        "total_tokens": int(resp_len * bs),
        "tokens_modified": 0,
        "good_steps": 0,
        "bad_steps": 0,
        "positive_samples": int(overall_pos.sum().item()),   # æ­£ sequence
        "negative_samples": int((~overall_pos).sum().item()),# è´Ÿ sequence
        "zero_adv_samples": 0,

        # æ–°å¢å››è±¡é™ç»Ÿè®¡
        "pos_good_steps": 0,
        "pos_bad_steps": 0,
        "neg_good_steps": 0,
        "neg_bad_steps": 0,

        # åŸºäº step åˆ†æ¡¶çš„ token æ•°
        "pos_tokens": 0,
        "neg_tokens": 0,
    }

    # é€æ ·æœ¬å¤„ç†ï¼ˆå†…éƒ¨ä»çŸ¢é‡åŒ– step_idï¼‰
    for b in tqdm(range(bs), desc="[vectorized_mask] Processing samples"):
        current_step_flags = step_flags[b]
        overall_adv_sum = overall_advs[b].item()

        # advantage=0 è·³è¿‡
        if abs(overall_adv_sum) < 1e-8:
            stats["zero_adv_samples"] += 1
            continue

        if not current_step_flags:
            continue

        sample_step_ids = step_ids[b]
        sample_overall_pos = bool(overall_pos[b].item())

        for step_id, is_good in enumerate(current_step_flags):
            step_mask = (sample_step_ids == step_id)
            if not step_mask.any():
                continue

            # scale factor
            if sample_overall_pos:
                factor = consistent_scale if is_good else pos_unconsistent_scale
            else:
                factor = neg_unconsistent_scale if is_good else consistent_scale

            scale[b].masked_fill_(step_mask, factor)

            tokens_in_step = int(step_mask.sum().item())
            stats["tokens_modified"] += tokens_in_step

            if is_good:
                stats["good_steps"] += 1
                if sample_overall_pos:
                    stats["pos_good_steps"] += 1
                else:
                    stats["neg_good_steps"] += 1
            else:
                stats["bad_steps"] += 1
                if sample_overall_pos:
                    stats["pos_bad_steps"] += 1
                else:
                    stats["neg_bad_steps"] += 1

            if sample_overall_pos:
                stats["pos_tokens"] += tokens_in_step
            else:
                stats["neg_tokens"] += tokens_in_step

    # padding token ç»´æŒ 1.0
    padding_mask = (step_ids == -1)
    scale.masked_fill_(padding_mask, 1.0)

    # åº”ç”¨
    original_adv_sum = adv.sum().item()
    batch.batch["advantages"] = adv * scale
    new_adv_sum = batch.batch["advantages"].sum().item()
    batch.batch["semantic_scale"] = scale

    # é¢å¤–ï¼šæŒ‰ advantage ç¬¦å·çš„åŸå§‹ token è®¡æ•°ï¼ˆçœ‹æ˜¯å¦è¢«è´Ÿæ ·æœ¬ domï¼‰
    valid_token_mask = response_mask & (~padding_mask)
    pos_token_mask = (adv > 0) & valid_token_mask
    neg_token_mask = (adv < 0) & valid_token_mask
    stats["pos_tokens_raw"] = int(pos_token_mask.sum().item())
    stats["neg_tokens_raw"] = int(neg_token_mask.sum().item())

    stats["original_adv_sum"] = original_adv_sum
    stats["new_adv_sum"] = new_adv_sum
    stats["adv_change_ratio"] = new_adv_sum / original_adv_sum if original_adv_sum != 0 else 1.0

    print(f"[vectorized_mask] Completed. Advantages: {original_adv_sum:.4f} -> {new_adv_sum:.4f}")
    print(f"[vectorized_mask] Modified {stats['tokens_modified']} tokens ({stats['good_steps']} good steps, {stats['bad_steps']} bad steps)")
    print(f"[vectorized_mask] Skipped {stats['zero_adv_samples']} samples with advantage=0")

    return stats


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åŒæ­¥åŒ…è£…å‡½æ•°ï¼ˆæ›´æ–°ä¸ºæ”¯æŒevaluation_typeå’Œapi_max_retriesï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def evaluate_step_flags(tokenizer,
                        batch,
                        good_words: tuple[str, ...] = ("GOOD",),
                        bad_words: tuple[str, ...] = ("BAD",),
                        model_name: str = "qwen-max",
                        evaluation_type: Literal["local", "api"] = "api",
                        use_parallel: bool = True,
                        max_concurrent: int = 20,
                        mask_tensor: torch.Tensor = None,
                        api_max_retries: int = 200,
                        save_dir: Optional[str] = None,
                        global_step: Optional[int] = None,
                        epoch: Optional[str] = None) -> List[List[bool]]:
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œå¯é€‰æ‹©ä½¿ç”¨å¹¶è¡Œæˆ–ä¸²è¡Œç‰ˆæœ¬ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè¯„ä¼°
    
    Args:
        tokenizer: åˆ†è¯å™¨
        batch: æ•°æ®æ‰¹æ¬¡
        good_words, bad_words: å…¼å®¹æ€§å‚æ•°ï¼Œåœ¨å¹¶è¡Œç‰ˆæœ¬ä¸­æœªä½¿ç”¨
        model_name: æ¨¡å‹åç§°
        evaluation_type: è¯„ä¼°ç±»å‹ï¼Œ"local"ä½¿ç”¨vLLMæœ¬åœ°æ¨¡å‹ï¼Œ"api"ä½¿ç”¨APIè°ƒç”¨
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
        api_max_retries: APIè°ƒç”¨çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç‰¹åˆ«ç”¨äºå¤„ç†429é”™è¯¯
        save_dir: ä¿å­˜è¯„ä¼°è®°å½•çš„ç›®å½•
        global_step: å…¨å±€æ­¥æ•°
        epoch: è®­ç»ƒè½®æ¬¡
    """
    if use_parallel:
        # ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œç‰ˆæœ¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        flags, stats = loop.run_until_complete(
            evaluate_step_flags_parallel(
                tokenizer=tokenizer,
                batch=batch,
                model_name=model_name,
                evaluation_type=evaluation_type,
                max_concurrent=max_concurrent,
                mask_tensor=mask_tensor,  # ä¼ å…¥å¤–éƒ¨mask
                api_max_retries=api_max_retries,  # ä¼ å…¥APIé‡è¯•æ¬¡æ•°
                save_dir=save_dir,  # ä¼ å…¥ä¿å­˜ç›®å½•
                global_step=global_step,  # ä¼ å…¥å…¨å±€æ­¥æ•°
                epoch=epoch  # ä¼ å…¥è®­ç»ƒè½®æ¬¡
            )
        )
        
        print(f"[evaluate_step_flags] Parallel execution stats: {stats}")
        return flags
    else:
        # ä½¿ç”¨åŸæ¥çš„ä¸²è¡Œç‰ˆæœ¬ï¼ˆéœ€è¦ä»åŸæ–‡ä»¶å¯¼å…¥ï¼‰
        print("[evaluate_step_flags] Using serial version (not implemented here)")
        raise NotImplementedError("Serial version not included in parallel implementation")

def apply_step_mask(batch,
                   step_flags: List[List[bool]],
                   consistent_scale: float = 1.0,
                   pos_unconsistent_scale: float = 0.2,
                   neg_unconsistent_scale: float = -0.2,
                   use_vectorized: bool = True,
                   mask_tensor: torch.Tensor = None):
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œå¯é€‰æ‹©ä½¿ç”¨å‘é‡åŒ–æˆ–åŸç‰ˆæœ¬
    
    Args:
        batch: æ‰¹æ¬¡æ•°æ®
        step_flags: stepè¯„ä¼°ç»“æœ
        consistent_scale, pos_unconsistent_scale, neg_unconsistent_scale: ç¼©æ”¾å› å­
        use_vectorized: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–ç‰ˆæœ¬
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
    """
    if use_vectorized:
        stats = apply_step_mask_vectorized(
            batch=batch,
            step_flags=step_flags,
            consistent_scale=consistent_scale,
            pos_unconsistent_scale=pos_unconsistent_scale,
            neg_unconsistent_scale=neg_unconsistent_scale,
            mask_tensor=mask_tensor  # ä¼ å…¥å¤–éƒ¨mask
        )
        return stats
    else:
        # ä½¿ç”¨åŸæ¥çš„ç‰ˆæœ¬ï¼ˆéœ€è¦ä»åŸæ–‡ä»¶å¯¼å…¥ï¼‰
        print("[apply_step_mask] Using original version (not implemented here)")
        raise NotImplementedError("Original version not included in vectorized implementation")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç»Ÿä¸€çš„å¤„ç†å™¨ç±»ï¼ˆæ”¯æŒevaluation_typeå’Œapi_max_retriesï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class ParallelSemanticProcessor:
    """å¹¶è¡Œè¯­ä¹‰å¤„ç†å™¨ï¼Œç”¨äºç®¡ç†æ•´ä¸ªæµç¨‹ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè¯„ä¼°"""
    
    def __init__(self, 
                 max_concurrent: int = 20,
                 batch_size_limit: int = 100,
                 model_name: str = "qwen-max",
                 evaluation_type: Literal["local", "api"] = "api",
                 api_max_retries: int = 200):
        self.max_concurrent = max_concurrent
        self.batch_size_limit = batch_size_limit
        self.model_name = model_name
        self.evaluation_type = evaluation_type
        self.api_max_retries = api_max_retries
        
        # æ ¹æ®è¯„ä¼°ç±»å‹è°ƒæ•´é»˜è®¤å‚æ•°
        if evaluation_type == "local":
            raise ValueError('Local evaluation via vLLM has been removed; set evaluation_type="api"')
            # æœ¬åœ°æ¨ç†å»ºè®®è¾ƒå°çš„å¹¶å‘æ•°å’Œæ‰¹æ¬¡å¤§å°
            if max_concurrent > 8:
                print(f"[ParallelSemanticProcessor] Local evaluation: reducing max_concurrent from {max_concurrent} to 8")
                self.max_concurrent = 8
            if batch_size_limit > 32:
                print(f"[ParallelSemanticProcessor] Local evaluation: reducing batch_size_limit from {batch_size_limit} to 32")
                self.batch_size_limit = 32
        
        print(f"[ParallelSemanticProcessor] Initialized with evaluation_type={evaluation_type}")
        print(f"[ParallelSemanticProcessor] Settings: model={model_name}, concurrent={self.max_concurrent}, batch_limit={self.batch_size_limit}, api_retries={self.api_max_retries}")
        
    async def process_batch(self, tokenizer, batch, 
                          consistent_scale: float = 1.0,
                          pos_unconsistent_scale: float = 0.2,
                          neg_unconsistent_scale: float = -0.2,
                          mask_tensor: torch.Tensor = None,
                          save_dir: Optional[str] = None,
                          global_step: Optional[int] = None,
                          epoch: Optional[str] = None) -> Dict:
        """
        å¤„ç†æ•´ä¸ªbatchçš„è¯­ä¹‰è¯„ä¼°å’Œmaskåº”ç”¨
        å¯¹äºadvantage=0çš„æ ·æœ¬ä¼šè·³è¿‡è¯„ä¼°
        
        Args:
            tokenizer: åˆ†è¯å™¨
            batch: æ‰¹æ¬¡æ•°æ®
            consistent_scale, pos_unconsistent_scale, neg_unconsistent_scale: ç¼©æ”¾å› å­
            mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensorï¼Œshape (bs, resp_len)
                        å¯ä»¥æ˜¯loss_maskæˆ–response_mask
            save_dir: ä¿å­˜è¯„ä¼°è®°å½•çš„ç›®å½•
            global_step: å…¨å±€æ­¥æ•°
            epoch: è®­ç»ƒè½®æ¬¡
        
        Returns:
            ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        # 1. å¹¶è¡Œè¯„ä¼°step flags
        eval_method = "vLLM" if self.evaluation_type == "local" else "API"
        print(f"[ParallelSemanticProcessor] Starting step evaluation with {eval_method}...")
        if save_dir:
            print(f"[ParallelSemanticProcessor] Evaluation records will be saved to: {save_dir}")
        eval_start = time.time()
        
        step_flags, eval_stats = await evaluate_step_flags_parallel(
            tokenizer=tokenizer,
            batch=batch,
            model_name=self.model_name,
            evaluation_type=self.evaluation_type,
            max_concurrent=self.max_concurrent,
            batch_size_limit=self.batch_size_limit,
            mask_tensor=mask_tensor,  # ä¼ å…¥å¤–éƒ¨mask
            api_max_retries=self.api_max_retries,  # ä¼ å…¥APIé‡è¯•æ¬¡æ•°
            save_dir=save_dir,  # ä¼ å…¥ä¿å­˜ç›®å½•
            global_step=global_step,  # ä¼ å…¥å…¨å±€æ­¥æ•°
            epoch=epoch  # ä¼ å…¥è®­ç»ƒè½®æ¬¡
        )
        
        eval_time = time.time() - eval_start
        print(f"[ParallelSemanticProcessor] Step evaluation completed in {eval_time:.2f}s")
        
        # 2. å‘é‡åŒ–åº”ç”¨mask
        print("[ParallelSemanticProcessor] Applying step mask...")
        mask_start = time.time()
        
        mask_stats = apply_step_mask_vectorized(
            batch=batch,
            step_flags=step_flags,
            consistent_scale=consistent_scale,
            pos_unconsistent_scale=pos_unconsistent_scale,
            neg_unconsistent_scale=neg_unconsistent_scale,
            mask_tensor=mask_tensor  # ä¼ å…¥å¤–éƒ¨mask
        )
        
        mask_time = time.time() - mask_start
        print(f"[ParallelSemanticProcessor] Step mask applied in {mask_time:.2f}s")
        
        # 3. åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        
        combined_stats = {
            "total_processing_time": total_time,
            "evaluation_time": eval_time,
            "mask_application_time": mask_time,
            "evaluation_stats": eval_stats,
            "mask_stats": mask_stats,
            "speedup_info": {
                "parallel_evaluation": True,
                "vectorized_masking": True,
                "max_concurrent": self.max_concurrent,
                "evaluation_type": self.evaluation_type,
                "using_vllm": self.evaluation_type == "local",
                "model_name": self.model_name,
                "api_max_retries": self.api_max_retries,
                "save_dir": save_dir
            }
        }
        
        print(f"[ParallelSemanticProcessor] Total processing time: {total_time:.2f}s")
        return combined_stats
    
    def process_batch_sync(self, tokenizer, batch, mask_tensor: torch.Tensor = None, 
                          save_dir: Optional[str] = None,
                          global_step: Optional[int] = None,
                          epoch: Optional[str] = None,
                          **kwargs) -> Dict:
        """
        åŒæ­¥ç‰ˆæœ¬çš„batchå¤„ç†
        
        Args:
            tokenizer: åˆ†è¯å™¨
            batch: æ‰¹æ¬¡æ•°æ®
            mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
            save_dir: ä¿å­˜è¯„ä¼°è®°å½•çš„ç›®å½•
            global_step: å…¨å±€æ­¥æ•°
            epoch: è®­ç»ƒè½®æ¬¡
            **kwargs: å…¶ä»–å‚æ•°
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            self.process_batch(tokenizer, batch, mask_tensor=mask_tensor, 
                             save_dir=save_dir, global_step=global_step, epoch=epoch, **kwargs)
        )